<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Payam Bahreyni" />


<title>Weight Lifting Prediction Assignment</title>

<script src="project_files/jquery-1.11.0/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="project_files/bootstrap-3.3.1/css/bootstrap.min.css" rel="stylesheet" />
<script src="project_files/bootstrap-3.3.1/js/bootstrap.min.js"></script>
<script src="project_files/bootstrap-3.3.1/shim/html5shiv.min.js"></script>
<script src="project_files/bootstrap-3.3.1/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="project_files/highlight/default.css"
      type="text/css" />
<script src="project_files/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img { 
  max-width:100%; 
  height: auto; 
}
</style>
<div class="container-fluid main-container">


<div id="header">
<h1 class="title">Weight Lifting Prediction Assignment</h1>
<h4 class="author"><em>Payam Bahreyni</em></h4>
<h4 class="date"><em>March 8, 2016</em></h4>
</div>


<div id="background" class="section level2">
<h2>Background</h2>
<p>Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.</p>
<p>In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <a href="http://groupware.les.inf.puc-rio.br/har" class="uri">http://groupware.les.inf.puc-rio.br/har</a>. [1]</p>
<p>[1] Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human ’13) . Stuttgart, Germany: ACM SIGCHI, 2013.</p>
</div>
<div id="data-description" class="section level2">
<h2>Data Description</h2>
<p>We have data gathered from 6 participants all doing the same exxercise. We know if they did the exercise the right way (classe: A) or wrong (other classes) as labeled by the experts (Classe variable in the training data). The measurements are gathered by sensors installed on the subjects’ arm, belt, forearm, and on the dumpbell.</p>
</div>
<div id="data-preparation-and-preprocessing" class="section level2">
<h2>Data Preparation and Preprocessing</h2>
<p>First, we need to load the data. We need to specify which strings should be considered as missing value, in order to load the measurements as numerical values.</p>
<pre class="r"><code>training &lt;- read.csv(&quot;./Project/data/pml-training.csv&quot;, na.strings = c(&quot;&quot;,&quot;#DIV/0!&quot;, &quot;NA&quot;))
testing &lt;- read.csv(&quot;./Project/data/pml-testing.csv&quot;, na.strings = c(&quot;&quot;,&quot;#DIV/0!&quot;, &quot;NA&quot;))

names(training[,1:10])</code></pre>
<pre><code>##  [1] &quot;X&quot;                    &quot;user_name&quot;            &quot;raw_timestamp_part_1&quot;
##  [4] &quot;raw_timestamp_part_2&quot; &quot;cvtd_timestamp&quot;       &quot;new_window&quot;          
##  [7] &quot;num_window&quot;           &quot;roll_belt&quot;            &quot;pitch_belt&quot;          
## [10] &quot;yaw_belt&quot;</code></pre>
<pre class="r"><code>library(ggplot2)

ggplot(training, aes(classe)) +
    geom_bar(fill= &quot;blue&quot;)</code></pre>
<p><img src="project_files/figure-html/unnamed-chunk-1-1.png" title="" alt="" width="672" /></p>
<p>Each classe is represented enough in the data set.</p>
<p>The first 7 variables are identifiers and configuration parameters. We need to separate the measurements from other variables in the data set.</p>
<pre class="r"><code>outcome &lt;- 160
irrelevant &lt;- c(1:7)

predictors &lt;- training[,-c(irrelevant,outcome)] </code></pre>
<div id="missing-value-treatment" class="section level3">
<h3>Missing Value Treatment</h3>
<p>Next, we examine the predictors for missing values. There are a lot of measurement variables that have a lot of missing values. These need to be left out in our prediction.</p>
<pre class="r"><code>percent.nas &lt;- sapply(predictors, function(x){round(sum(is.na(x)) * 100/length(x), 2)})
percent.nas[percent.nas &gt; 0]</code></pre>
<pre><code>##       kurtosis_roll_belt      kurtosis_picth_belt        kurtosis_yaw_belt 
##                    97.98                    98.09                   100.00 
##       skewness_roll_belt     skewness_roll_belt.1        skewness_yaw_belt 
##                    97.98                    98.09                   100.00 
##            max_roll_belt           max_picth_belt             max_yaw_belt 
##                    97.93                    97.93                    97.98 
##            min_roll_belt           min_pitch_belt             min_yaw_belt 
##                    97.93                    97.93                    97.98 
##      amplitude_roll_belt     amplitude_pitch_belt       amplitude_yaw_belt 
##                    97.93                    97.93                    97.98 
##     var_total_accel_belt            avg_roll_belt         stddev_roll_belt 
##                    97.93                    97.93                    97.93 
##            var_roll_belt           avg_pitch_belt        stddev_pitch_belt 
##                    97.93                    97.93                    97.93 
##           var_pitch_belt             avg_yaw_belt          stddev_yaw_belt 
##                    97.93                    97.93                    97.93 
##             var_yaw_belt            var_accel_arm             avg_roll_arm 
##                    97.93                    97.93                    97.93 
##          stddev_roll_arm             var_roll_arm            avg_pitch_arm 
##                    97.93                    97.93                    97.93 
##         stddev_pitch_arm            var_pitch_arm              avg_yaw_arm 
##                    97.93                    97.93                    97.93 
##           stddev_yaw_arm              var_yaw_arm        kurtosis_roll_arm 
##                    97.93                    97.93                    98.33 
##       kurtosis_picth_arm         kurtosis_yaw_arm        skewness_roll_arm 
##                    98.34                    97.99                    98.32 
##       skewness_pitch_arm         skewness_yaw_arm             max_roll_arm 
##                    98.34                    97.99                    97.93 
##            max_picth_arm              max_yaw_arm             min_roll_arm 
##                    97.93                    97.93                    97.93 
##            min_pitch_arm              min_yaw_arm       amplitude_roll_arm 
##                    97.93                    97.93                    97.93 
##      amplitude_pitch_arm        amplitude_yaw_arm   kurtosis_roll_dumbbell 
##                    97.93                    97.93                    97.96 
##  kurtosis_picth_dumbbell    kurtosis_yaw_dumbbell   skewness_roll_dumbbell 
##                    97.94                   100.00                    97.95 
##  skewness_pitch_dumbbell    skewness_yaw_dumbbell        max_roll_dumbbell 
##                    97.94                   100.00                    97.93 
##       max_picth_dumbbell         max_yaw_dumbbell        min_roll_dumbbell 
##                    97.93                    97.96                    97.93 
##       min_pitch_dumbbell         min_yaw_dumbbell  amplitude_roll_dumbbell 
##                    97.93                    97.96                    97.93 
## amplitude_pitch_dumbbell   amplitude_yaw_dumbbell       var_accel_dumbbell 
##                    97.93                    97.96                    97.93 
##        avg_roll_dumbbell     stddev_roll_dumbbell        var_roll_dumbbell 
##                    97.93                    97.93                    97.93 
##       avg_pitch_dumbbell    stddev_pitch_dumbbell       var_pitch_dumbbell 
##                    97.93                    97.93                    97.93 
##         avg_yaw_dumbbell      stddev_yaw_dumbbell         var_yaw_dumbbell 
##                    97.93                    97.93                    97.93 
##    kurtosis_roll_forearm   kurtosis_picth_forearm     kurtosis_yaw_forearm 
##                    98.36                    98.36                   100.00 
##    skewness_roll_forearm   skewness_pitch_forearm     skewness_yaw_forearm 
##                    98.35                    98.36                   100.00 
##         max_roll_forearm        max_picth_forearm          max_yaw_forearm 
##                    97.93                    97.93                    98.36 
##         min_roll_forearm        min_pitch_forearm          min_yaw_forearm 
##                    97.93                    97.93                    98.36 
##   amplitude_roll_forearm  amplitude_pitch_forearm    amplitude_yaw_forearm 
##                    97.93                    97.93                    98.36 
##        var_accel_forearm         avg_roll_forearm      stddev_roll_forearm 
##                    97.93                    97.93                    97.93 
##         var_roll_forearm        avg_pitch_forearm     stddev_pitch_forearm 
##                    97.93                    97.93                    97.93 
##        var_pitch_forearm          avg_yaw_forearm       stddev_yaw_forearm 
##                    97.93                    97.93                    97.93 
##          var_yaw_forearm 
##                    97.93</code></pre>
<pre class="r"><code># Filter highly-NA from predictors
ncol(predictors)</code></pre>
<pre><code>## [1] 152</code></pre>
<pre class="r"><code>predictors &lt;- predictors[, -which(percent.nas &gt; 0)]
ncol(predictors)</code></pre>
<pre><code>## [1] 52</code></pre>
</div>
<div id="predictor-correlation" class="section level3">
<h3>Predictor Correlation</h3>
<p>For the remaining predictors, we need to know how they are correlated to each other.</p>
<pre class="r"><code>M &lt;- abs(cor(predictors))
diag(M) &lt;- 0

# Array containing highly-correlated vars
arr &lt;- which(M&gt;.7, arr.ind=T)

# Data frame containing correlated vars
cor.df &lt;- data.frame(row.no= arr[,1], row= names(predictors)[arr[,1]], 
                     col.no= arr[,2], col= names(predictors)[arr[,2]])
cor.df$corr &lt;- mapply(function(i,j) { M[i,j] }, cor.df$row.no, cor.df$col.no)
    #M[cor.df$row.no, cor.df$col.no]

# Keep one of the instances of each pair
library(dplyr)</code></pre>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;
## 
## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag
## 
## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>nrow(cor.df)</code></pre>
<pre><code>## [1] 76</code></pre>
<pre class="r"><code>cor.df &lt;- filter(cor.df, row.no &gt; col.no)
nrow(cor.df)</code></pre>
<pre><code>## [1] 38</code></pre>
<pre class="r"><code># visualize correlation btw correlated variables                     
ggplot(cor.df, aes(row, col, color= corr)) +
    geom_point() +
    theme(axis.text.x= element_text(angle= 45, vjust=1, hjust= 1))</code></pre>
<p><img src="project_files/figure-html/unnamed-chunk-4-1.png" title="" alt="" width="672" /></p>
<p>Because of the movement measurements are highly correlated, we will Principal Component Analysis to get smaller number of uncorrelated predictors to work with.</p>
</div>
<div id="principal-component-analysis-pca" class="section level3">
<h3>Principal Component Analysis (PCA)</h3>
<p>First we will get the principal components using <code>prcomp()</code> function. Examining the scree plot, we see that 7 components is all we need to explain the most variation.</p>
<pre class="r"><code>pcs.pr &lt;- prcomp(predictors, scale. = T)
plot(pcs.pr, type=&quot;l&quot;)</code></pre>
<p><img src="project_files/figure-html/unnamed-chunk-5-1.png" title="" alt="" width="672" /></p>
<p>Then, we use <code>caret</code> package to calculate the components for training and test data.</p>
<pre class="r"><code>library(caret)</code></pre>
<pre><code>## Warning: package &#39;caret&#39; was built under R version 3.2.3</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre class="r"><code>PC.model &lt;- preProcess(predictors, method = &quot;pca&quot;, pcaComp= 7)
# Calculate training Principle Components
train.pcs &lt;- predict(PC.model, predictors)

test.pcs &lt;- predict(PC.model, newdata = testing)
test.pcs.ncols &lt;- ncol(test.pcs)
test.pcs &lt;- test.pcs[,(test.pcs.ncols-6):(test.pcs.ncols)]</code></pre>
</div>
</div>
<div id="predictive-modeling" class="section level2">
<h2>Predictive Modeling</h2>
<p>We are ready to use the principal components to predict the Classes of activity. Since we would like to get an accurate prediction and not really interested in model interpretability, I will use a Random Forest model to predict.</p>
<div id="cross-validation" class="section level3">
<h3>Cross Validation</h3>
<p>We will use k-fold cross validation with k= 10. I didn’t want to use a high K to avoid overfitting.</p>
<pre class="r"><code>train.data &lt;- cbind(train.pcs, training$classe)
names(train.data)[ncol(train.data)] &lt;- &quot;classe&quot;

tr &lt;- trainControl(method= &quot;cv&quot;, number = 10)
model &lt;- train(classe ~ ., method= &quot;rf&quot;, data= train.data, trControl = tr)</code></pre>
<pre><code>## Loading required package: randomForest
## randomForest 4.6-12
## Type rfNews() to see new features/changes/bug fixes.
## 
## Attaching package: &#39;randomForest&#39;
## 
## The following object is masked from &#39;package:dplyr&#39;:
## 
##     combine</code></pre>
<pre class="r"><code>model</code></pre>
<pre><code>## Random Forest 
## 
## 19622 samples
##     7 predictor
##     5 classes: &#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 17661, 17660, 17660, 17661, 17659, 17658, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
##   2     0.9405777  0.9248187  0.004777331  0.006047638
##   4     0.9376215  0.9210780  0.004597549  0.005830103
##   7     0.9239124  0.9037121  0.004524121  0.005760392
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 2.</code></pre>
</div>
<div id="prediction" class="section level3">
<h3>Prediction</h3>
<p>Given the in-sample error rate, I expect the prediction to be about 90% accurate. Now, we use the model we got in the last step to predict classe for the test data.</p>
<pre class="r"><code>test.classe &lt;- predict(model, newdata = test.pcs)</code></pre>
<pre><code>## Loading required package: randomForest
## randomForest 4.6-12
## Type rfNews() to see new features/changes/bug fixes.
## 
## Attaching package: &#39;randomForest&#39;
## 
## The following object is masked from &#39;package:dplyr&#39;:
## 
##     combine</code></pre>
<pre class="r"><code>test.classe</code></pre>
<pre><code>##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E</code></pre>
<pre class="r"><code>str(test.classe)</code></pre>
<pre><code>##  Factor w/ 5 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,..: 2 1 2 1 1 5 4 2 1 1 ...</code></pre>
<pre class="r"><code>result &lt;- data.frame(problem_id= testing$problem_id, classe=test.classe)

result</code></pre>
<pre><code>##    problem_id classe
## 1           1      B
## 2           2      A
## 3           3      B
## 4           4      A
## 5           5      A
## 6           6      E
## 7           7      D
## 8           8      B
## 9           9      A
## 10         10      A
## 11         11      B
## 12         12      C
## 13         13      B
## 14         14      A
## 15         15      E
## 16         16      E
## 17         17      A
## 18         18      B
## 19         19      B
## 20         20      B</code></pre>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
